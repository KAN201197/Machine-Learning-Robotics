{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "completed test gym with car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 14:19:11.347129: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-23 14:19:11.762065: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-23 14:19:11.765235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 14:19:12.926820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from Gym_new import ImprovedLTYEnv\n",
    "import time\n",
    "from ACNet import ActorNetwork, CriticNetwork\n",
    "from Utilities import normalize_observation, discount_rewards_2, calculate_gaes, plot_learning_curve\n",
    "from PPO_Agent import PPOAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, \n",
    "                 actor_model, critic_model, \n",
    "                 env, map='map3',\n",
    "                 max_steps=200, num_episodes=300,\n",
    "                 print_freq=10, batch_size=5,\n",
    "                 epochs=4, update_frequency=10,\n",
    "                 epsilon=0.2, target_kl_div=0.01,\n",
    "                 max_policy_iters=1, max_value_iters=1,\n",
    "                 policy_lr=0.0003, value_lr=0.0003,\n",
    "                 max_grad_norm=None, train=False,\n",
    "                 checkpoint_dir='new_models/'):\n",
    "\n",
    "\n",
    "        # Init Params\n",
    "        self.max_steps = max_steps\n",
    "        self.num_episodes = num_episodes\n",
    "        self.print_freq = print_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.update_frequency = update_frequency\n",
    "        self.epsilon =  epsilon\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_iters = max_policy_iters\n",
    "        self.max_value_iters = max_value_iters\n",
    "        self.policy_lr = policy_lr\n",
    "        self.value_lr = value_lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.train = train\n",
    "        self.map = map\n",
    "        #self.checkpoint_dir=checkpoint_dir\n",
    "\n",
    "\n",
    "        # Init Models and Environment\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model =  critic_model\n",
    "        self.env = env\n",
    "\n",
    "        if self.train:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        else:\n",
    "            self.checkpoint_dir = 'models/'\n",
    "    \n",
    "        # Initialize PPO Agent\n",
    "        self.ppo = PPOAgent(self.actor_model,\n",
    "                            self.critic_model,\n",
    "                            self.epsilon,\n",
    "                            self.target_kl_div,\n",
    "                            self.max_policy_iters,\n",
    "                            self.max_value_iters,\n",
    "                            self.policy_lr,\n",
    "                            self.value_lr,\n",
    "                            self.max_grad_norm,\n",
    "                            checkpoint_dir=self.checkpoint_dir)\n",
    "    \n",
    "    def work(self):\n",
    "        if self.train:\n",
    "            num_steps = 0\n",
    "            ep_rewards = []\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            max_ep_reward = 0\n",
    "            pl = 0\n",
    "            vl = 0\n",
    "\n",
    "            for episode in range(self.num_episodes):\n",
    "                obs = np.array(self.env.reset())\n",
    "                done = False\n",
    "                goal = False\n",
    "                loop_count = 0\n",
    "                ep_reward = 0\n",
    "\n",
    "                while not done:\n",
    "                    loop_count += 1\n",
    "                    obs = normalize_observation(obs)\n",
    "\n",
    "                    action, value, log_prob = self.ppo.get_action_value(obs)\n",
    "\n",
    "                    new_obs, reward, done, goal, info = self.env.step(np.squeeze(action))\n",
    "\n",
    "                    num_steps += 1\n",
    "\n",
    "                    ep_reward += reward\n",
    "\n",
    "                    self.ppo.store_memory(obs, np.squeeze(action), np.squeeze(value), np.squeeze(log_prob), reward)\n",
    "\n",
    "                    # Online Training\n",
    "                    if num_steps % self.update_frequency == 0:\n",
    "\n",
    "                        for epoch in range(self.epochs):\n",
    "                            \n",
    "                            # Access the memory\n",
    "                            states, actions, values, old_log_probs, rewards = self.ppo.access_memory()\n",
    "\n",
    "                            # Calculate advantages\n",
    "                            advantages = calculate_gaes(rewards, values)\n",
    "\n",
    "                            # Calculate returns\n",
    "                            returns = discount_rewards_2(advantages, values)\n",
    "\n",
    "                            # Shuffle the data and generate batches\n",
    "                            n_states = len(states)\n",
    "                            batch_start = np.arange(0, n_states, self.batch_size)\n",
    "                            indices = np.arange(n_states, dtype=np.int64)\n",
    "                            np.random.shuffle(indices)\n",
    "                            batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "                            for batch in batches:\n",
    "                                pl = self.ppo.train_policy(states[batch], actions[batch], advantages[batch], old_log_probs[batch])\n",
    "                                vl = self.ppo.train_value(states[batch], returns[batch])\n",
    "\n",
    "                        self.ppo.clear_memory()\n",
    "                    \n",
    "                    if loop_count >= self.max_steps:\n",
    "                        break\n",
    "\n",
    "                    obs = np.array(new_obs)\n",
    "\n",
    "                # Store losses after every episode\n",
    "                policy_losses.append(pl.numpy())\n",
    "                value_losses.append(vl.numpy())\n",
    "\n",
    "                # Saves the model weight if the model reaches the goal\n",
    "                if goal:\n",
    "                    if ep_reward >= max_ep_reward:\n",
    "                        max_ep_reward = ep_reward\n",
    "                        self.ppo.save_models()\n",
    "                \n",
    "                ep_rewards.append(ep_reward)\n",
    "\n",
    "                if (episode +1) % self.print_freq == 0:\n",
    "                    print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "                        episode + 1, np.mean(ep_rewards[-self.print_freq:])\n",
    "                    ))\n",
    "\n",
    "            # Plot Rewards, Policy Loss and Value Loss\n",
    "            x = [i+1 for i in range(len(ep_rewards))]\n",
    "            plot_learning_curve(x, ep_rewards)\n",
    "            plt.ylabel('Total Rewards')\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.show()\n",
    "\n",
    "            x = [i+1 for i in range(len(policy_losses))]\n",
    "            plot_learning_curve(x, policy_losses)\n",
    "            plt.ylabel('Policy Loss')\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.show()\n",
    "\n",
    "            x = [i+1 for i in range(len(value_losses))]\n",
    "            plot_learning_curve(x, value_losses)\n",
    "            plt.ylabel('Value Loss')\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            env = ImprovedLTYEnv(map=self.map)\n",
    "            self.ppo.load_models()\n",
    "\n",
    "            for _ in range(10):\n",
    "                obs = np.array(env.reset())\n",
    "                done = False\n",
    "                goal = False\n",
    "                total_reward = 0\n",
    "                step = 0\n",
    "\n",
    "                while not done:\n",
    "\n",
    "                    obs = normalize_observation(obs)\n",
    "                    action, value, log_prob = self.ppo.get_action_value(obs)\n",
    "                    new_obs, reward, done, goal, info = env.step(np.squeeze(action))\n",
    "                    total_reward = total_reward + reward\n",
    "                    env.render()\n",
    "                    time.sleep(0.01)\n",
    "                    step +=1\n",
    "                    obs = np.array(new_obs)\n",
    "                \n",
    "                print(\"no. step : \", step)\n",
    "                print(\" \")\n",
    "                print(\"total reward :\", total_reward)\n",
    "\n",
    "            env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kent/.local/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....loading_models....\n",
      "We have reached goal.\n",
      "no. step :  156\n",
      " \n",
      "total reward : 1003.2574206068216\n",
      "no. step :  156\n",
      " \n",
      "total reward : 302.5381948179604\n",
      "We have reached goal.\n",
      "no. step :  153\n",
      " \n",
      "total reward : 702.7608104569317\n",
      "We have reached goal.\n",
      "no. step :  155\n",
      " \n",
      "total reward : 1004.3727052265406\n",
      "We have reached goal.\n",
      "no. step :  152\n",
      " \n",
      "total reward : 702.9112717116923\n",
      "no. step :  156\n",
      " \n",
      "total reward : 303.74487334853416\n",
      "We have reached goal.\n",
      "no. step :  156\n",
      " \n",
      "total reward : 1004.6289124450683\n",
      "no. step :  153\n",
      " \n",
      "total reward : 304.0649804366231\n",
      "We have reached goal.\n",
      "no. step :  291\n",
      " \n",
      "total reward : 1438.2303991474805\n",
      "We have reached goal.\n",
      "no. step :  154\n",
      " \n",
      "total reward : 702.237750428021\n"
     ]
    }
   ],
   "source": [
    "# Define Actor and Critic Models and Necessary params to provide to class Trainer\n",
    "action_space = 2\n",
    "hidden_layer_size = 32\n",
    "num_hidden_layers_actor = 2\n",
    "num_hidden_layers_critic = 3\n",
    "\n",
    "actor_model = ActorNetwork(action_space, hidden_layer_size, num_hidden_layers_actor)\n",
    "critic_model = CriticNetwork(hidden_layer_size, num_hidden_layers_critic)\n",
    "\n",
    "env = env = ImprovedLTYEnv()\n",
    "\n",
    "# Map options: Option1: map='map1a' Option2: map='map3' (Default)\n",
    "# Necessary Params: 1. actor_model, 2. critic_model, 3. env\n",
    "# Optional Params: Refer to the __init__ () constructor in Trainer Class\n",
    "# The trained model works on both the maps. Set train=False to load the pre-trained model. Select the map to test the model\n",
    "  \n",
    "trainer = Trainer(actor_model, critic_model, env, train=False, map='map3')\n",
    "\n",
    "trainer.work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
